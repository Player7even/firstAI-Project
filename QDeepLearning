import numpy as np
import pickle
import random
from collections import defaultdict


class TicTacToeEnv:
    def __init__(self):
        self.discovered_patterns = set()  # Neue Variable für entdeckte Muster
        self.reset()

    def reset(self):
        self.board = np.zeros((3, 3), dtype=int)
        self.current_player = 1
        return self.get_state()

    def print_board(self):
        symbols = {0: " ", 1: "1", 2: "0"}
        print("\nSpielfeld:")
        print("-------------")
        for i in range(3):
            print("|", end=" ")
            for j in range(3):
                print(f"{symbols[self.board[i][j]]}", end=" | ")
            print("\n-------------")

    def get_state(self):
        return str(self.board.flatten().tolist())

    def get_valid_moves(self):
        return [(i, j) for i in range(3) for j in range(3) if self.board[i][j] == 0]

    def evaluate_position(self):
        for pattern in self.discovered_patterns:
            values = [self.board[i][j] for i, j in pattern]
            if values.count(self.current_player) == 2 and values.count(0) == 1:
                return 20
            if values.count(3 - self.current_player) == 2 and values.count(0) == 1:
                return -20
        return 0

    def make_move(self, move):
        i, j = move
        if self.board[i][j] != 0:
            return self.get_state(), -200, True

        self.board[i][j] = self.current_player

        # Prüfe auf Gewinner nur wenn mindestens 5 Züge gemacht wurden
        moves_made = np.count_nonzero(self.board)
        if moves_made >= 5:  # Erst ab 5 Zügen ist ein Sieg möglich
            winner = self.check_winner()
            if winner:
                return self.get_state(), 200 if winner == self.current_player else -200, True

        # Prüfe auf Unentschieden nur wenn alle Felder belegt sind
        if len(self.get_valid_moves()) == 0:
            return self.get_state(), 0, True

        position_value = self.evaluate_position()
        self.current_player = 3 - self.current_player
        return self.get_state(), position_value, False

    def check_winner(self):
        # Prüfe alle möglichen 3er-Kombinationen auf dem Brett
        for i in range(3):
            for j in range(3):
                # Horizontale Prüfung
                if j == 0 and self.board[i][j] != 0:
                    if all(self.board[i][k] == self.board[i][j] for k in range(3)):
                        pattern = tuple((i, k) for k in range(3))
                        self.discovered_patterns.add(pattern)
                        return self.board[i][j]

                # Vertikale Prüfung
                if i == 0 and self.board[i][j] != 0:
                    if all(self.board[k][j] == self.board[i][j] for k in range(3)):
                        pattern = tuple((k, j) for k in range(3))
                        self.discovered_patterns.add(pattern)
                        return self.board[i][j]

        # Diagonale Prüfung (von links oben nach rechts unten)
        if self.board[0][0] != 0 and all(self.board[i][i] == self.board[0][0] for i in range(3)):
            pattern = tuple((i, i) for i in range(3))
            self.discovered_patterns.add(pattern)
            return self.board[0][0]

        # Diagonale Prüfung (von rechts oben nach links unten)
        if self.board[0][2] != 0 and all(self.board[i][2-i] == self.board[0][2] for i in range(3)):
            pattern = tuple((i, 2-i) for i in range(3))
            self.discovered_patterns.add(pattern)
            return self.board[0][2]

        return None


class QLearningAgent:
    def __init__(self, learning_rate=0.2, discount_factor=0.99, epsilon=0.3):
        self.q_table = {}
        self.lr = learning_rate
        self.gamma = discount_factor
        self.epsilon = epsilon
        self.min_epsilon = 0.01
        self.epsilon_decay = 0.9995
        self.winning_moves = {}
        self.pattern_weights = {}  # Neue Variable für Mustergewichtung

    def get_state_actions(self, state):
        if state not in self.q_table:
            self.q_table[state] = {}
        return self.q_table[state]

    def get_action(self, state, valid_moves):
        if random.random() < self.epsilon:
            return random.choice(valid_moves)
        return self.get_best_action(state, valid_moves)

    def get_best_action(self, state, valid_moves):
        state_actions = self.get_state_actions(state)
        best_value = float('-inf')
        best_actions = []

        # Analysiere das aktuelle Brett
        board = np.array(eval(state)).reshape(3, 3)
        current_player = 1 if np.sum(board == 1) == np.sum(board == 2) else 2

        # Prüfe auf unmittelbare Gewinnmöglichkeiten
        for move in valid_moves:
            # Simuliere den Zug
            test_board = board.copy()
            test_board[move[0]][move[1]] = current_player

            # Prüfe horizontale Gewinnmöglichkeit
            for i in range(3):
                if np.sum(test_board[i] == current_player) == 3:
                    return move

            # Prüfe vertikale Gewinnmöglichkeit
            for j in range(3):
                if np.sum(test_board[:, j] == current_player) == 3:
                    return move

            # Prüfe diagonale Gewinnmöglichkeiten
            if np.sum(np.diag(test_board) == current_player) == 3:
                return move
            if np.sum(np.diag(np.fliplr(test_board)) == current_player) == 3:
                return move

        # Prüfe auf Blockademöglichkeiten (Gegner am Gewinnen hindern)
        opponent = 3 - current_player
        for move in valid_moves:
            test_board = board.copy()
            test_board[move[0]][move[1]] = opponent

            # Horizontale Bedrohung
            for i in range(3):
                if np.sum(test_board[i] == opponent) == 3:
                    return move

            # Vertikale Bedrohung
            for j in range(3):
                if np.sum(test_board[:, j] == opponent) == 3:
                    return move

            # Diagonale Bedrohungen
            if np.sum(np.diag(test_board) == opponent) == 3:
                return move
            if np.sum(np.diag(np.fliplr(test_board)) == opponent) == 3:
                return move

        # Bewerte Züge basierend auf gelernten Mustern und aktueller Situation
        for move in valid_moves:
            move_str = str(move)
            base_value = state_actions.get(move_str, 0.0)
            pattern_value = 0
            position_value = 0

            # Strategische Positionen bewerten
            if move == (1, 1):  # Zentrum
                position_value += 3
            elif move in [(0, 0), (0, 2), (2, 0), (2, 2)]:  # Ecken
                position_value += 2

            # Bewerte Muster-basiert
            for pattern_key, weight in self.pattern_weights.items():
                pattern = eval(pattern_key)
                if move in pattern:
                    # Prüfe, wie viele Steine bereits in diesem Muster sind
                    pattern_positions = [board[pos[0]][pos[1]] for pos in pattern]
                    own_stones = pattern_positions.count(current_player)
                    opponent_stones = pattern_positions.count(opponent)

                    if own_stones == 2 and opponent_stones == 0:
                        pattern_value += weight * 2
                    elif own_stones == 1 and opponent_stones == 0:
                        pattern_value += weight

            # Kombiniere alle Bewertungsfaktoren
            total_value = base_value + pattern_value * 0.1 + position_value

            # Berücksichtige auch die Nähe zu eigenen Steinen
            for i in range(3):
                for j in range(3):
                    if board[i][j] == current_player:
                        distance = abs(move[0] - i) + abs(move[1] - j)
                        if distance == 1:  # Direkt angrenzend
                            total_value += 1

            if total_value > best_value:
                best_value = total_value
                best_actions = [move]
            elif total_value == best_value:
                best_actions.append(move)

        # Wenn alle Bewertungen gleich sind, bevorzuge strategische Positionen
        if len(best_actions) == len(valid_moves):
            preferred_moves = [(1, 1), (0, 0), (0, 2), (2, 0), (2, 2)]
            for move in preferred_moves:
                if move in valid_moves:
                    return move

        return random.choice(best_actions)

    def learn(self, state, action, reward, next_state, valid_next_moves):
        if state not in self.q_table:
            self.q_table[state] = {}
        if str(action) not in self.q_table[state]:
            self.q_table[state][str(action)] = 0.0

        next_state_actions = self.get_state_actions(next_state)
        best_next_value = 0.0
        if valid_next_moves:
            best_next_value = max(
                [next_state_actions.get(str(move), 0.0) for move in valid_next_moves]
            )

        current_value = self.q_table[state][str(action)]
        new_value = current_value + self.lr * (
                reward + self.gamma * best_next_value - current_value
        )
        self.q_table[state][str(action)] = new_value

    def learn_pattern(self, state, action, reward, env):
        if reward == 200:  # Wenn ein Gewinnzug gemacht wurde
            # Speichere das Muster, das zum Gewinn geführt hat
            for pattern in env.discovered_patterns:
                pattern_key = str(pattern)
                if pattern_key not in self.pattern_weights:
                    self.pattern_weights[pattern_key] = 1
                else:
                    self.pattern_weights[pattern_key] += 1

    def decay_epsilon(self):
        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)

    def save(self, filename):
        try:
            with open(filename, 'wb') as f:
                pickle.dump({
                    'q_table': self.q_table,
                    'winning_moves': self.winning_moves,
                    'pattern_weights': self.pattern_weights
                }, f)
        except Exception as e:
            print(f"Fehler beim Speichern: {e}")

    def load(self, filename):
        try:
            with open(filename, 'rb') as f:
                data = pickle.load(f)
                self.q_table = data['q_table']
                self.winning_moves = data['winning_moves']
                self.pattern_weights = data.get('pattern_weights', {})  # Kompatibilität mit älteren Versionen
        except (FileNotFoundError, EOFError):
            self.q_table = {}
            self.winning_moves = {}
            self.pattern_weights = {}
            raise


def train_ai(episodes=10000):
    env = TicTacToeEnv()
    agent = QLearningAgent()

    try:
        agent.load('.pkl')
        print("Bestehendes Modell geladen.")
    except (FileNotFoundError, EOFError):
        print("Neues Modell wird erstellt.")

    wins = {1: 0, 2: 0, 'draw': 0}
    total_episodes = episodes
    discovered_patterns_count = 0

    print(f"Starte Training für {total_episodes} Episoden...")

    for episode in range(total_episodes):
        state = env.reset()
        done = False
        moves_history = []

        while not done:
            valid_moves = env.get_valid_moves()
            action = agent.get_action(state, valid_moves)
            next_state, reward, done = env.make_move(action)
            valid_next_moves = env.get_valid_moves()

            # Speichere den Spielverlauf
            moves_history.append((state, action, env.current_player))

            # Lerne aus dem aktuellen Zug
            agent.learn(state, action, reward, next_state, valid_next_moves)

            state = next_state

            if done:
                winner = env.check_winner()
                if winner:
                    wins[winner] += 1
                    # Lerne aus dem Gewinnmuster
                    for pattern in env.discovered_patterns:
                        if pattern not in agent.pattern_weights:
                            agent.pattern_weights[str(pattern)] = 1
                            discovered_patterns_count += 1
                        else:
                            agent.pattern_weights[str(pattern)] += 1

                    # Verstärke die Gewinnstrategie
                    for old_state, move, player in moves_history:
                        if player == winner:
                            agent.learn(old_state, move, 200, next_state, [])
                            agent.learn_pattern(old_state, move, 200, env)
                else:
                    wins['draw'] += 1

        agent.decay_epsilon()

        if episode % 100 == 0:
            print(f"\nEpisode {episode}/{total_episodes} ({(episode / total_episodes) * 100:.1f}%)")
            print(f"Statistiken - Siege P1: {wins[1]}, Siege P2: {wins[2]}, Unentschieden: {wins['draw']}")
            print(f"Aktuelle Exploration Rate (Epsilon): {agent.epsilon:.3f}")
            print(f"Entdeckte Muster: {discovered_patterns_count}")
            print(f"Top-5 erfolgreichste Muster:")
            # Zeige die erfolgreichsten Muster
            sorted_patterns = sorted(agent.pattern_weights.items(), key=lambda x: x[1], reverse=True)[:5]
            for pattern, weight in sorted_patterns:
                print(f"Muster: {pattern}, Gewicht: {weight}")
            agent.save('tictactoe_ai.pkl')

        if episode % 1000 == 999:
            agent.save('tictactoe_ai.pkl')
            print(f"\nZwischenstand gespeichert nach Episode {episode + 1}")
            # Behalte die Gesamtstatistik bei
            # wins = {1: 0, 2: 0, 'draw': 0}  # Diese Zeile wurde entfernt

    agent.save('tictactoe_ai.pkl')
    print("\nTraining abgeschlossen und Modell gespeichert.")
    print(f"Finale Statistiken - Siege P1: {wins[1]}, Siege P2: {wins[2]}, Unentschieden: {wins['draw']}")
    print(f"Insgesamt entdeckte Muster: {discovered_patterns_count}")
    print("\nTop-10 erfolgreichste Muster:")
    sorted_patterns = sorted(agent.pattern_weights.items(), key=lambda x: x[1], reverse=True)[:10]
    for pattern, weight in sorted_patterns:
        print(f"Muster: {pattern}, Gewicht: {weight}")
    return agent

def play_against_ai():
    env = TicTacToeEnv()
    agent = QLearningAgent()
    agent.epsilon = 0  # Deaktiviere Exploration im Spielmodus

    try:
        agent.load('tictactoe_ai.pkl')
        print("KI-Modell geladen.")
    except (FileNotFoundError, EOFError):
        print("Kein trainiertes Modell gefunden. Starte Training...")
        agent = train_ai()

    games_played = 0
    human_wins = 0
    ai_wins = 0
    draws = 0

    while True:
        state = env.reset()
        done = False
        moves_history = []

        while not done:
            env.print_board()
            current_state = state
            valid_moves = env.get_valid_moves()

            if env.current_player == 1:
                print("\nDu bist am Zug (Spieler 1)")
                while True:
                    try:
                        row = int(input("Zeile (0-2): "))
                        col = int(input("Spalte (0-2): "))
                        if (row, col) in valid_moves:
                            break
                        print("Ungültiger Zug!")
                    except ValueError:
                        print("Bitte gib gültige Zahlen ein!")
                action = (row, col)
            else:
                print("\nKI ist am Zug (Spieler 2)")
                action = agent.get_best_action(state, valid_moves)
                print(f"KI wählt: Zeile {action[0]}, Spalte {action[1]}")

            moves_history.append((current_state, action, env.current_player))
            next_state, reward, done = env.make_move(action)
            state = next_state

            if done:
                env.print_board()
                winner = env.check_winner()
                games_played += 1

                if winner == 1:
                    human_wins += 1
                    print("\nGlückwunsch! Du hast gewonnen!")
                    # Lerne aus menschlichen Gewinnzügen
                    for i, (old_state, move, player) in enumerate(moves_history):
                        if player == 1:
                            agent.learn(old_state, move, 200 * (0.9 ** (len(moves_history) - i)),
                                        state, env.get_valid_moves())
                            agent.learn_pattern(old_state, move, 200, env)
                        else:
                            agent.learn(old_state, move, -200 * (0.9 ** (len(moves_history) - i)),
                                        state, env.get_valid_moves())

                elif winner == 2:
                    ai_wins += 1
                    print("\nDie KI hat gewonnen!")
                    # Verstärke erfolgreiche KI-Strategie
                    for i, (old_state, move, player) in enumerate(moves_history):
                        if player == 2:
                            agent.learn(old_state, move, 200 * (0.9 ** (len(moves_history) - i)),
                                        state, env.get_valid_moves())
                            agent.learn_pattern(old_state, move, 200, env)

                else:
                    draws += 1
                    print("\nUnentschieden!")
                    # Neutrale Bewertung für alle Züge
                    for old_state, move, _ in moves_history:
                        agent.learn(old_state, move, 50, state, env.get_valid_moves())

                # Speichere das gelernte Modell
                agent.save('tictactoe_ai.pkl')
                print("\nKI hat aus diesem Spiel gelernt.")
                print(f"\nSpielstatistiken:")
                print(f"Gespielte Spiele: {games_played}")
                print(f"Mensch: {human_wins} Siege")
                print(f"KI: {ai_wins} Siege")
                print(f"Unentschieden: {draws}")

                # Debug-Information
                print("\nGelernte Zustände:", len(agent.q_table))
                if moves_history:
                    last_move = moves_history[-1]
                    print(
                        f"Letzter Zug Q-Wert: {agent.q_table.get(last_move[0], {}).get(str(last_move[1]), 'Nicht gefunden')}")

                # Zeige die Top-5 gewichteten Muster
                if agent.pattern_weights:
                    print("\nTop-5 gelernte Muster:")
                    sorted_patterns = sorted(agent.pattern_weights.items(), key=lambda x: x[1], reverse=True)[:5]
                    for pattern, weight in sorted_patterns:
                        print(f"Muster: {pattern}, Gewicht: {weight}")

        if input("\nNoch einmal spielen? (j/n): ").lower() != 'j':
            break

    print("\nFinale Statistiken:")
    print(f"Gespielte Spiele: {games_played}")
    print(f"Mensch: {human_wins} Siege ({(human_wins / games_played) * 100:.1f}%)")
    print(f"KI: {ai_wins} Siege ({(ai_wins / games_played) * 100:.1f}%)")
    print(f"Unentschieden: {draws} ({(draws / games_played) * 100:.1f}%)")
    print(f"Finale Anzahl gelernter Muster: {len(env.discovered_patterns)}")


def check_save_status(filename='tictactoe_ai.pkl'):
    try:
        with open(filename, 'rb') as f:
            try:
                data = pickle.load(f)
                q_table = data['q_table']
                winning_moves = data['winning_moves']
                print(f"\nGespeicherte Daten gefunden:")
                print(f"Anzahl gespeicherter Zustände: {len(q_table)}")
                total_actions = sum(len(actions) for actions in q_table.values())
                print(f"Gesamtzahl gespeicherter Aktionen: {total_actions}")
                if len(q_table) > 0:
                    avg_actions = total_actions / len(q_table)
                    print(f"Durchschnittliche Aktionen pro Zustand: {avg_actions:.2f}")
            except EOFError:
                print(f"\nDie Datei '{filename}' ist leer oder beschädigt.")
    except FileNotFoundError:
        print(f"\nKeine gespeicherte Datei '{filename}' gefunden.")
    except Exception as e:
        print(f"\nFehler beim Laden der Datei: {e}")


def main():
    while True:
        print("\n1. KI trainieren")
        print("2. Gegen KI spielen")
        print("3. Speicherstatus prüfen")
        print("4. Beenden")

        choice = input("\nWähle eine Option (1-4): ")

        if choice == '1':
            while True:
                try:
                    episodes = int(input("Anzahl der Trainingsepisoden (mindestens 1000 empfohlen): "))
                    if episodes < 100:
                        print("Bitte mindestens 100 Episoden für effektives Training wählen.")
                        continue
                    break
                except ValueError:
                    print("Bitte eine gültige Zahl eingeben.")
            train_ai(episodes)
        elif choice == '2':
            play_against_ai()
        elif choice == '3':
            check_save_status()
        elif choice == '4':
            break
        else:
            print("Ungültige Eingabe!")


if __name__ == "__main__":
    main()
